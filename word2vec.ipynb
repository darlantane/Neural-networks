{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88962b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c787fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train)\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62d1782b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23497668,  0.21410768, -0.09329746,  0.28843257, -0.04212553,\n",
       "       -0.34174207,  0.1387363 ,  0.6288712 , -0.20370507, -0.18113895,\n",
       "       -0.09912767, -0.34090236, -0.00578235,  0.12549575, -0.03556455,\n",
       "       -0.19897054,  0.19211519, -0.23955515,  0.01756367, -0.35183042,\n",
       "        0.10034265,  0.07496872,  0.21786611, -0.09130505,  0.05712227,\n",
       "        0.01246604, -0.22605494, -0.15108247, -0.14810516,  0.02504244,\n",
       "        0.18676655, -0.02100156,  0.17425832, -0.31824365, -0.09372737,\n",
       "        0.26894048,  0.11289704, -0.1902635 , -0.2051254 , -0.35592568,\n",
       "       -0.20140712, -0.23146987, -0.23238839,  0.19963369,  0.3337569 ,\n",
       "        0.00120562, -0.1379078 , -0.08244433,  0.25404862,  0.14747573,\n",
       "        0.09173041, -0.11931178, -0.24258114,  0.01009944, -0.19944556,\n",
       "        0.14878528,  0.11641503, -0.07407069, -0.21641934,  0.07837123,\n",
       "        0.09931982,  0.03796556, -0.02898055,  0.09036348, -0.22365566,\n",
       "        0.17359386, -0.08948612,  0.14312512, -0.312696  ,  0.30790636,\n",
       "       -0.13523464,  0.19370084,  0.3056533 , -0.18462399,  0.3388892 ,\n",
       "        0.12631352,  0.0422069 ,  0.06625129, -0.18760148,  0.01160624,\n",
       "       -0.15531388, -0.18611187, -0.16854793,  0.20169829, -0.14686172,\n",
       "       -0.14995779, -0.02356982,  0.24905422,  0.30890638,  0.10683078,\n",
       "        0.20791589,  0.20847104,  0.04619349,  0.04532569,  0.42493898,\n",
       "        0.11539716,  0.22341576, -0.20869514,  0.00744912,  0.04453827],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d9b8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding space is of size 100\n"
     ]
    }
   ],
   "source": [
    "size = len(wv['cat'])\n",
    "\n",
    "print(f'The embedding space is of size {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ffc6f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hotel', 0.9776648283004761),\n",
       " ('wedding', 0.9737498760223389),\n",
       " ('letters', 0.9731951951980591),\n",
       " ('door', 0.9720884561538696),\n",
       " ('revenge', 0.9708735942840576),\n",
       " ('mate', 0.9703019261360168),\n",
       " ('breasts', 0.9700096845626831),\n",
       " ('apartment', 0.9693183898925781),\n",
       " ('attend', 0.9692969918251038),\n",
       " ('storm', 0.9689933657646179)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37fc8eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 1.0),\n",
       " ('hotel', 0.9776648283004761),\n",
       " ('wedding', 0.9737498760223389),\n",
       " ('letters', 0.9731951951980591),\n",
       " ('door', 0.9720884561538696),\n",
       " ('revenge', 0.9708735942840576),\n",
       " ('mate', 0.9703019261360168),\n",
       " ('breasts', 0.9700096845626831),\n",
       " ('apartment', 0.9693183898925781),\n",
       " ('attend', 0.9692969918251038)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding = wv['car']\n",
    "wv.similar_by_vector(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eb999de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38255894, -0.10721177,  0.2304197 ,  0.02755582,  0.13324553,\n",
       "       -0.39485073, -0.1168834 ,  0.44016442, -0.05154616, -0.57956505,\n",
       "       -0.09273262, -0.07966226,  0.5475657 ,  0.07492611, -0.06786942,\n",
       "        0.21997654, -0.37384874,  0.23065782, -0.2574172 ,  0.15629458,\n",
       "       -0.14411569, -0.05025716,  0.09265947,  0.1087577 ,  0.19852541,\n",
       "       -0.05322106, -0.20482506,  0.5323632 , -0.08989573, -0.531576  ,\n",
       "       -0.3143677 ,  0.28394455, -0.00750485,  0.05733454, -0.12472564,\n",
       "       -0.25121814,  0.03185821, -0.2427192 , -0.32929644,  0.92004144,\n",
       "       -0.3644895 , -0.03496844, -0.3654007 , -0.08925325,  0.36635056,\n",
       "       -0.3328017 ,  0.41363016, -0.38901007,  0.0645439 ,  0.42165482,\n",
       "        0.08537297, -0.01028484, -0.35063824, -0.15137872,  0.23469532,\n",
       "        0.30082825,  0.08236474,  0.03962462,  0.25411546,  0.8313312 ,\n",
       "        0.14700057,  0.29615444, -0.22249144, -0.41692823,  0.37150043,\n",
       "       -0.05624306, -0.3431719 , -0.3161069 ,  0.65464365,  0.16249269,\n",
       "       -0.5589508 , -0.5096579 ,  0.1533326 ,  0.36240196, -0.13299596,\n",
       "        0.03390411,  0.01114811, -0.03893495,  0.2758705 ,  0.11465578,\n",
       "        0.08023845,  0.03294401, -0.30200672, -0.18104017, -0.32192314,\n",
       "        0.3586961 , -0.0419327 , -0.17882836,  0.28768235, -0.7576439 ,\n",
       "       -0.60126364, -0.23761773,  0.40072754,  0.36346826, -0.41332334,\n",
       "       -0.33399135,  0.09554908,  0.4889302 , -0.7375249 ,  0.07106555],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['good'] - wv['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4035cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wv['good'] - wv['bad'] + wv['stupid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "268e85df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.8048093914985657),\n",
       " ('good', 0.7815244197845459),\n",
       " ('great', 0.771703839302063),\n",
       " ('decent', 0.7659851908683777),\n",
       " ('such', 0.7566556334495544),\n",
       " ('always', 0.7431327104568481),\n",
       " ('given', 0.7388507127761841),\n",
       " ('potential', 0.7275189161300659),\n",
       " ('also', 0.7275111675262451),\n",
       " ('tight', 0.7247037887573242)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c115b7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 0.9623402953147888),\n",
       " ('performance', 0.8658728003501892),\n",
       " ('actress', 0.8373754024505615),\n",
       " ('role', 0.8170179724693298),\n",
       " ('job', 0.8021565079689026),\n",
       " ('guy', 0.767738401889801),\n",
       " ('character', 0.7672237753868103),\n",
       " ('gunslinger', 0.7588706612586975),\n",
       " ('man', 0.7535820007324219),\n",
       " ('touchy', 0.7453116774559021)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = wv['queen'] - wv['king'] + wv['actor']\n",
    "wv.similar_by_vector(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75ff9014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_2 = Word2Vec(sentences=X_train, vector_size=50)\n",
    "wv2 = word2vec_2.wv\n",
    "len(wv2['movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6be89720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 8006\n",
      "Number of different words in the train set 30419\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size', len(wv2.key_to_index))\n",
    "\n",
    "diff_words = set([_ for elt in X_train for _ in elt])\n",
    "print('Number of different words in the train set', len(diff_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "099416ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word in W2V #1 : 8006\n",
      "Number of word in W2V #2 : 8006\n",
      "Number of word in W2V #3 : 1385\n",
      "Number of word in W2V #4 : 16729\n"
     ]
    }
   ],
   "source": [
    "word2vec_3 = Word2Vec(sentences=X_train, vector_size=50, min_count=40)\n",
    "word2vec_4 = Word2Vec(sentences=X_train, vector_size=50, min_count=2)\n",
    "\n",
    "print(f'Number of word in W2V #1 : {len(wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #2 : {len(wv2.key_to_index)}')\n",
    "print(f'Number of word in W2V #3 : {len(word2vec_3.wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #4 : {len(word2vec_4.wv.key_to_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0fc1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_5 = Word2Vec(sentences=X_train, vector_size=50, min_count=40, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b40cc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = ['this', 'movie', 'is', 'the', 'worst', 'action', 'movie', 'ever']\n",
    "example_missing_words = ['this', 'movie', 'is', 'laaaaaaaaaame']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (8, 100))\n",
    "\n",
    "embedded_sentence_missing_words = embed_sentence(word2vec, example_missing_words)\n",
    "assert(type(embedded_sentence_missing_words) == np.ndarray)\n",
    "assert(embedded_sentence_missing_words.shape == (3, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
